{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc8f33f8-fd62-4a69-8309-12fcf3d0f103",
   "metadata": {},
   "source": [
    "# [Model Compression](https://www.cs.cornell.edu/~caruana/compression.kdd06.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04eadac2-b37a-4c4b-bb22-729c22d692a3",
   "metadata": {},
   "source": [
    "## Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de98cd9b-7e62-4cc9-ab42-b44bf1baba8f",
   "metadata": {},
   "source": [
    "* **Gap**: In some cases it is not enough for a model to be highly accurate, but also has to meet strict time and space requirements. However, the best performing models are usually large and slow, while the fast and compact models are less accurate.\n",
    "* **Improvement**: A neural network can mimic the function learned by an ensemble. This is accomplished by first assigning labels to the unlabeled data with the ensemble's predictions. The neural network is then trained on this extended dataset. The neural network is able to achieve similar performance while being 1000x smaller and faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc268172-98e7-4b9a-8be2-432302e492b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911c68b4-8b52-43f8-92d8-7627d647e159",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26384213-e353-4fa4-8bf1-91b35ea02d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/AirlineSatisfaction/prepared.csv')\n",
    "target_column = \"isSatisfied\"\n",
    "X = df.drop(columns=[target_column])\n",
    "y = df[target_column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf0b7165-0c2b-43b0-93e5-4a3486aab8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, X_unlabeled, y, y_unlabeled = train_test_split(X, y, test_size=0.1, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842db9af-df19-42dc-bbf1-c6788d93db9b",
   "metadata": {},
   "source": [
    "## Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a429acd5-e401-4b66-9a39-ebcce055f39d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\angel\\anaconda3\\envs\\efficiency\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 11.37410151\n",
      "Iteration 2, loss = 7.32667968\n",
      "Iteration 3, loss = 6.62887457\n",
      "Iteration 4, loss = 4.69135660\n",
      "Iteration 5, loss = 6.45999439\n",
      "Iteration 6, loss = 3.68290125\n",
      "Iteration 7, loss = 5.28672309\n",
      "Iteration 8, loss = 5.35402399\n",
      "Iteration 9, loss = 4.95053401\n",
      "Iteration 10, loss = 3.55727248\n",
      "Iteration 11, loss = 5.74648083\n",
      "Iteration 12, loss = 4.45347841\n",
      "Iteration 13, loss = 4.01830633\n",
      "Iteration 14, loss = 4.82615149\n",
      "Iteration 15, loss = 4.25791396\n",
      "Iteration 16, loss = 4.69685177\n",
      "Iteration 17, loss = 4.81809310\n",
      "Iteration 18, loss = 4.25753665\n",
      "Iteration 19, loss = 5.35811330\n",
      "Iteration 20, loss = 4.21231975\n",
      "Iteration 21, loss = 5.15376243\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Ensemble Model Accuracy: 0.9127\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "gb = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "lr = LogisticRegression(max_iter=500)\n",
    "# svm = SVC(probability=True, max_iter=1000, random_state=42)\n",
    "knn = KNeighborsClassifier()\n",
    "nn = MLPClassifier(hidden_layer_sizes=(64,), max_iter=250, random_state=42, verbose=True)\n",
    "\n",
    "ensemble = VotingClassifier(estimators=[('rf', rf), ('gb', gb), ('lr', lr), ('knn', knn), ('nn', nn)], voting='soft')\n",
    "\n",
    "ensemble.fit(X_train, y_train)\n",
    "\n",
    "y_pred = ensemble.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Ensemble Model Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16096da9-9457-44a4-bdb0-0eac1fc0a222",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3346eccb-df09-4111-b5e8-de3a1e156e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "26bfaa78-65f6-4c87-9151-f0151dce3835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network model\n",
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BinaryClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(23, 5)  # 23 features to 64 units\n",
    "        self.fc2 = nn.Linear(5, 1)  # 64 units to 1 unit\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))  # ReLU activation after first layer\n",
    "        x = self.fc2(x)              # No activation here, since we'll apply sigmoid in loss\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4631afbd-c0e6-4dde-a603-c5db8c344a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BinaryClassifier()\n",
    "criterion = nn.BCEWithLogitsLoss()  # Suitable for multi-class/binary classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2bcf0d46-6c79-43b1-8beb-ada1d708e0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to tensor\n",
    "X_train_tensor = torch.tensor(X_train.to_numpy(), dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.to_numpy(), dtype=torch.float32)\n",
    "\n",
    "# Create DataLoader for batching\n",
    "dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f413a8ae-c119-4677-b0f7-222f5312f722",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 0.34164809515843025\n",
      "Epoch [2/20], Loss: 0.2697305103143056\n",
      "Epoch [3/20], Loss: 0.25815077085270843\n",
      "Epoch [4/20], Loss: 0.2491979707764764\n",
      "Epoch [5/20], Loss: 0.24076046637999707\n",
      "Epoch [6/20], Loss: 0.229980640571851\n",
      "Epoch [7/20], Loss: 0.2216198118069233\n",
      "Epoch [8/20], Loss: 0.21552911436455882\n",
      "Epoch [9/20], Loss: 0.20998504518443703\n",
      "Epoch [10/20], Loss: 0.20746769293760642\n",
      "Epoch [11/20], Loss: 0.20467149634391835\n",
      "Epoch [12/20], Loss: 0.20337636689854482\n",
      "Epoch [13/20], Loss: 0.2018525931570265\n",
      "Epoch [14/20], Loss: 0.19998608691315364\n",
      "Epoch [15/20], Loss: 0.1985171880859595\n",
      "Epoch [16/20], Loss: 0.19794716728039277\n",
      "Epoch [17/20], Loss: 0.19689710854719847\n",
      "Epoch [18/20], Loss: 0.19580699935173376\n",
      "Epoch [19/20], Loss: 0.19479812843422603\n",
      "Epoch [20/20], Loss: 0.19416763638584023\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()  # Clear the gradients\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Compute loss (CrossEntropyLoss already applies softmax internally)\n",
    "        loss = criterion(outputs, labels.unsqueeze(1))\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Print loss for every epoch\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "93a7f8ae-1350-40ed-b626-8b94f9f40a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to tensor\n",
    "X_test_tensor = torch.tensor(X_test.to_numpy(), dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.to_numpy(), dtype=torch.float32)\n",
    "\n",
    "# Create DataLoader for batching\n",
    "dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1ee99211-7bb3-441d-b4ae-8fed01817192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 92.11%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "model.eval()  # Set the model to evaluation mode (turns off dropout, batchnorm, etc.)\n",
    "\n",
    "with torch.no_grad():  # No need to calculate gradients during evaluation\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Apply softmax to get class probabilities (CrossEntropyLoss internally uses softmax)\n",
    "        preds = nn.functional.sigmoid(outputs) > 0.5\n",
    "        \n",
    "        # Count correct predictions\n",
    "        correct += (preds.squeeze() == labels.squeeze()).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "accuracy = correct / total * 100\n",
    "print(f'Accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60817902-f963-48c5-b1c6-e143ab9e4ead",
   "metadata": {},
   "source": [
    "## Distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "86a90904-4c10-4c25-a4a5-c83c8e4e51ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_unlabeled_pred = ensemble.predict(X_unlabeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c0bda8c5-0925-4c76-b2d7-24aac9aab122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Model Accuracy: 0.9571\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_unlabeled, y_unlabeled_pred)\n",
    "print(f\"Ensemble Model Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2d8c8181-8e73-4da7-8e11-cd4169e32702",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_new = np.concatenate([X_train.to_numpy(), X_unlabeled.to_numpy()])\n",
    "y_train_new = np.concatenate([y_train.to_numpy(), y_unlabeled_pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "10a7bb81-541a-4f7e-be7f-d0571825cfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BinaryClassifier()\n",
    "criterion = nn.BCEWithLogitsLoss()  # Suitable for multi-class/binary classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "24efd3b7-731f-48e3-9b82-d331ef5f16f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to tensor\n",
    "X_train_tensor = torch.tensor(X_train_new, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_new, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoader for batching\n",
    "dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "863cb571-00f9-45cd-8683-3ab17ac0379d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 0.2940400201860849\n",
      "Epoch [2/20], Loss: 0.1887842782803842\n",
      "Epoch [3/20], Loss: 0.17141822811674784\n",
      "Epoch [4/20], Loss: 0.16301834965879852\n",
      "Epoch [5/20], Loss: 0.15718934070822355\n",
      "Epoch [6/20], Loss: 0.15329945792217511\n",
      "Epoch [7/20], Loss: 0.15042429366470636\n",
      "Epoch [8/20], Loss: 0.14892071888253494\n",
      "Epoch [9/20], Loss: 0.14738787536707906\n",
      "Epoch [10/20], Loss: 0.14636520295775868\n",
      "Epoch [11/20], Loss: 0.1455314329156915\n",
      "Epoch [12/20], Loss: 0.1451026990383237\n",
      "Epoch [13/20], Loss: 0.1444618572500554\n",
      "Epoch [14/20], Loss: 0.1438194095036826\n",
      "Epoch [15/20], Loss: 0.1429416001424775\n",
      "Epoch [16/20], Loss: 0.14319644922631103\n",
      "Epoch [17/20], Loss: 0.1424717718215139\n",
      "Epoch [18/20], Loss: 0.14307365318139395\n",
      "Epoch [19/20], Loss: 0.14244857138475855\n",
      "Epoch [20/20], Loss: 0.14176064166422184\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()  # Clear the gradients\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Compute loss (CrossEntropyLoss already applies softmax internally)\n",
    "        loss = criterion(outputs, labels.unsqueeze(1))\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Print loss for every epoch\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f3fcacb2-8de6-44df-9bb6-4d1236423975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to tensor\n",
    "X_test_tensor = torch.tensor(X_test.to_numpy(), dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.to_numpy(), dtype=torch.float32)\n",
    "\n",
    "# Create DataLoader for batching\n",
    "dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "339ee173-5d69-4f87-bc0d-a6f89c154fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 93.88%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "model.eval()  # Set the model to evaluation mode (turns off dropout, batchnorm, etc.)\n",
    "\n",
    "with torch.no_grad():  # No need to calculate gradients during evaluation\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Apply softmax to get class probabilities (CrossEntropyLoss internally uses softmax)\n",
    "        preds = nn.functional.sigmoid(outputs) > 0.5\n",
    "        \n",
    "        # Count correct predictions\n",
    "        correct += (preds.squeeze() == labels.squeeze()).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "accuracy = correct / total * 100\n",
    "print(f'Accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5199bd35-8a06-4200-87dc-538de7e8f099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble: 0.9555\n",
    "# Small Neural Net: 0.9211\n",
    "# Small Neural Net w/ Model Compression: 0.9388\n",
    "\n",
    "# around 2% increase in performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
