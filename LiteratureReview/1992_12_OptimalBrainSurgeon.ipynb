{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18487608",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Optimal Brain Surgeon (OBS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a381a35-45f6-4559-b7d0-464a06f2b18d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96483c65-7fc6-4601-859b-30f8691c3f60",
   "metadata": {},
   "source": [
    "**Which parameters should be pruned in a neural network?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff8f5c5-531f-4f32-ac44-cc16eb49d1c6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Current gap**: Magnitude pruning and OBD prune based on weight size or diagonal curvature, which ignores interactions between weights. This often leads to removing important weights and requires retraining to recover performance.\n",
    "\n",
    "**Improvement:** OBS uses the full Hessian inverse to evaluate true weight importance and applies a compensation update after pruning. This preserves the networkâ€™s function, avoids pruning the wrong weights, and eliminates the need for retraining."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2ea239-065d-400a-8a85-a81b61a780b6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba4eeb8-4f70-4def-af68-0693fd6b7516",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "1. Train a \"reasonably\" large network to minimum error \n",
    "2. Compute inverse of Hessian\n",
    "3. Find q that gives smallest saliency. If the increase in error from such removal is much smaller than E, then delete and continue. Otherwise, go to 5.\n",
    "4. Use q from previous step to update all the weights. Go to step 2.\n",
    "5. No more weights can be deleted without large increase in E. (At this point it may be desirable to retrain the network). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d35a86-f8a4-4488-af62-98b98839dbcc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d111c51-314b-4030-879d-9f442586a7d1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Finds right weights to remove, whereas sometimes OBD finds incorrect weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2049e9c8-5eb6-4b5a-9787-a001145efa52",
   "metadata": {},
   "source": [
    "## Optimization Math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b4985e-66af-4b6f-8961-d65f34a9bdfc",
   "metadata": {},
   "source": [
    "$$\n",
    "\\delta E \n",
    "= \\left( \\frac{\\partial E}{\\partial w} \\right)^{\\!\\top} \n",
    "\\delta w \n",
    "+ \\frac{1}{2} \\delta w^\\top H \\, \\delta w \n",
    "+ \\mathcal{O}(\\|\\delta w\\|^3)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\delta E \n",
    "= \\cancel{\\left( \\frac{\\partial E}{\\partial w} \\right)^{\\!\\top} \\delta w}\n",
    "+ \\frac{1}{2} \\delta w^\\top H \\, \\delta w \n",
    "+ \\cancel{\\mathcal{O}(\\|\\delta w\\|^3)}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebd5371-3f6b-4e56-b817-fa2c94d8dc4f",
   "metadata": {},
   "source": [
    "$$\n",
    "\\min_{\\delta w} \\;\\; \\frac{1}{2}\\,\\delta w^\\top H \\,\\delta w\n",
    "\\qquad \\text{subject to} \\qquad \\text{we remove one weight completely}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\delta w_q = -\\,w_q \n",
    "\\qquad\\text{or equivalently}\\qquad\n",
    "e_q^\\top \\delta w = -\\,w_q\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\min_{\\delta w} \\;\\; \\frac{1}{2}\\,\\delta w^\\top H \\,\\delta w\n",
    "\\qquad \\text{subject to} \\qquad\n",
    "e_q^\\top \\delta w + w_q = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathcal{L} \n",
    "= \\frac{1}{2} \\delta w^\\top H \\, \\delta w \n",
    "\\;+\\; \\lambda \\left( e_q^\\top \\delta w + w_q \\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2b0f43-335b-4c69-ac7c-cae57d641f6c",
   "metadata": {},
   "source": [
    "Take gradient w.r.t. $\\delta w$:\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial (\\delta w)} \n",
    "= H \\delta w + \\lambda e_q = 0\n",
    "$$\n",
    "\n",
    "Thus,\n",
    "$$\n",
    "\\delta w = -\\lambda H^{-1} e_q \\tag{1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae27af58-c56b-46cb-a06e-4293e0999fca",
   "metadata": {},
   "source": [
    "Now take gradient w.r.t. $\\lambda$:\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\lambda}\n",
    "= e_q^\\top \\delta w + w_q = 0\n",
    "$$\n",
    "\n",
    "Substitute (1):\n",
    "$$\n",
    "e_q^\\top (-\\lambda H^{-1} e_q) + w_q = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "-\\lambda \\, e_q^\\top H^{-1} e_q + w_q = 0\n",
    "$$\n",
    "\n",
    "Thus,\n",
    "$$\n",
    "\\lambda = -\\frac{w_q}{e_q^\\top H^{-1} e_q} \\tag{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d27be0-d861-4cff-8768-67bceec3bea8",
   "metadata": {},
   "source": [
    "Plug (2) into (1):\n",
    "$$\n",
    "\\delta w\n",
    "= \\frac{w_q H^{-1} e_q}{e_q^\\top H^{-1} e_q}\n",
    "= \\frac{w_q H^{-1} e_q}{(H^{-1})_{qq}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c914849e-73d7-44c8-98fd-5682f77c00ca",
   "metadata": {},
   "source": [
    "Now compute loss increase $L_q$:\n",
    "$$\n",
    "L_q \n",
    "= \\frac{1}{2} \\delta w^\\top H \\delta w \n",
    "+ \\lambda( e_q^\\top \\delta w + w_q )\n",
    "$$\n",
    "\n",
    "Constraint is zero, so:\n",
    "$$\n",
    "L_q = \\frac{1}{2} \\delta w^\\top H \\delta w\n",
    "$$\n",
    "\n",
    "Substitute $\\delta w = \\frac{w_q}{(H^{-1})_{qq}} H^{-1} e_q$:\n",
    "$$\n",
    "L_q \n",
    "= \\frac{1}{2}\n",
    "\\left( \n",
    "\\frac{w_q H^{-1} e_q}{(H^{-1})_{qq}}\n",
    "\\right)^\\top\n",
    "H\n",
    "\\left( \n",
    "\\frac{w_q H^{-1} e_q}{(H^{-1})_{qq}}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "Factor out $\\frac{w_q^2}{(H^{-1})_{qq}^2}$:\n",
    "$$\n",
    "L_q \n",
    "= \\frac{w_q^2}{2 (H^{-1})_{qq}^2} \n",
    "\\left( e_q^\\top H^{-1} H H^{-1} e_q \\right)\n",
    "$$\n",
    "\n",
    "Since $H^{-1} H H^{-1} = H^{-1} \\text{ and } H^{-1}$ is symmetric:\n",
    "$$\n",
    "L_q\n",
    "= \\frac{w_q^2}{2 (H^{-1})_{qq}^2}\n",
    "\\left( e_q^\\top H^{-1} e_q \\right)\n",
    "$$\n",
    "\n",
    "But:\n",
    "$$\n",
    "e_q^\\top H^{-1} e_q = (H^{-1})_{qq}\n",
    "$$\n",
    "\n",
    "Therefore:\n",
    "$$\n",
    "L_q = \n",
    "\\frac{1}{2} \\frac{w_q^2}{(H^{-1})_{qq}}\n",
    "$$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
