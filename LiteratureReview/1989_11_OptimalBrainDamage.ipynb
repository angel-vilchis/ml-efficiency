{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18487608",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Optimal Brain Damage (OBD)](https://proceedings.neurips.cc/paper_files/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a381a35-45f6-4559-b7d0-464a06f2b18d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909386ae-68f2-4568-9374-1309bbdc7ae2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Performance-complexity tradeoff** (aka bias-variance tradeoff): Too complex models do not generalize well enough and too simple models don't represent data well enough. \n",
    "\n",
    "One way to address tradeoff is by adding complexity penalty in objective function:\n",
    "\n",
    "$$\n",
    "\\text{Objective}\n",
    "= \\underbrace{\\text{Loss term}}_{\\text{Data fit}}\n",
    "+ \\lambda\\,\\underbrace{\\text{Complexity term}}_{\\text{Regularization}}\n",
    "$$\n",
    "\n",
    "Assuming more parameters increases complexity and less parameters decreases complexity, i.e. **number of parameters $\\propto$ complexity**, another way to address the performance-complexity tradeoff is by **pruning** unimportant parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eab8808-5043-49e4-be7e-d1fe272fe5f9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Once the parameters to prune are decided, pruning can be applied in different ways: \n",
    "* Can set the parameters to zero (**hard magnitude pruning**) OR drastically decrease the parameters, but keep non-zero (**soft magnitude pruning**)\n",
    "* Can freeze the weights after applying the decrease (**hard-prune**) OR can allow them to adapt when re-training the model (**soft-prune**)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4772e675-cd3c-45e9-aa23-bbcb67fdd9c1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Benefits of  **pruning** include:\n",
    "* Better generalization\n",
    "* Sample efficiency\n",
    "* Improved training and inference speed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41e9446-5803-4ba2-977e-cb27747675df",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Deciding which parameters to prune is not as straightforward in neural networks as it is in other models with priori structural information, e.g. when deciding which parameters to prune in polynomial regression model-the higher order terms are usually chosen first since they mostly capture fine-grained fluctuations rather than the general trend. \n",
    "\n",
    "**Which parameters should be pruned in a neural network?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca2372f-a545-4ef8-8772-956bea6ab65b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Prior to OBD, neural network pruning has been mostly **magnitude-based**:\n",
    "    The smallest parameters should be removed.\n",
    "\n",
    "However, magnitude-based pruning doesn’t truly measure a parameter’s **saliency** (a parameters importance to the objective). A parameter might be small yet crucial for the network’s predictions. A brute-force but conceptually accurate alternative would be to measure saliency by removing one parameter at a time, retraining, and observing the change in loss—though this is computationally impractical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff8f5c5-531f-4f32-ac44-cc16eb49d1c6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Current gap**: No pruning method accurately and efficiently measures each weight’s real impact on the loss — earlier approaches were either conceptually inaccurate (magnitude pruning) or computationally expensive (brute-force saliency).\n",
    "\n",
    "**Improvement**: OBD introduces a practical, saliency-based pruning method that can safely remove a large fraction of weights by using the second order derivatives of the objective function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2ea239-065d-400a-8a85-a81b61a780b6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba4eeb8-4f70-4def-af68-0693fd6b7516",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "1. Train a neural network until reasonable solution obtained \n",
    "2. Compute the second derivatives for each parameter \n",
    "3. Compute the saliencies for each parameter \n",
    "4. Sort the parameters by saliency and delete some low-saliency parameters \n",
    "5. Iterate\n",
    "\n",
    "OBD can be also be applied as interactive tool: graphing saliencies and deciding what to prune at a higher level, such as a whole layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d35a86-f8a4-4488-af62-98b98839dbcc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15470f97",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Removing up to 60% of parameters without change in performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f478182c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2049e9c8-5eb6-4b5a-9787-a001145efa52",
   "metadata": {},
   "source": [
    "## Saliency Math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61f500b-548d-4d09-bf6d-ba360b680e6f",
   "metadata": {},
   "source": [
    "#### Recall Taylor-Series\n",
    "$$\n",
    "f(a+\\Delta x)\n",
    "= f(a)\n",
    "+ f'(a)\\,\\Delta x\n",
    "+ \\frac{f''(a)}{2!}(\\Delta x)^2\n",
    "+ \\frac{f^{(3)}(a)}{3!}(\\Delta x)^3\n",
    "+ \\frac{f^{(4)}(a)}{4!}(\\Delta x)^4\n",
    "+ \\cdots\n",
    "+ \\frac{f^{(n)}(a)}{n!}(\\Delta x)^n\n",
    "+ O\\!\\left((\\Delta x)^{n+1}\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094fe23e-2745-4953-b6ec-d5ba2a9ec91a",
   "metadata": {},
   "source": [
    "#### Taylor-Series applied to weights ($\\vec{w}$) and objective function ($E$)\n",
    "$$\n",
    "E(\\vec{w} + \\Delta \\vec{w}) = E(\\vec{w}) \n",
    "+ \\nabla E(\\vec{w})^{T} \\Delta \\vec{w} \n",
    "+ \\tfrac{1}{2} \\Delta \\vec{w}^{T} H(\\vec{w}) \\Delta \\vec{w}\n",
    "+ O(\\|\\Delta \\vec{w}\\|^3)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0189af0-3e70-4539-bd98-c964509bea56",
   "metadata": {},
   "source": [
    "#### Changing weights ($\\vec{w}$) by  $\\Delta \\vec{w}$ changes $E$ by $\\Delta E$\n",
    "$$\n",
    "\\underbrace{E(\\vec{w} + \\Delta \\vec{w}) - E(\\vec{w})}_{\\Delta E}\n",
    "= \\nabla E(\\vec{w})^{T} \\Delta \\vec{w} \n",
    "+ \\tfrac{1}{2} \\Delta \\vec{w}^{T} H(\\vec{w}) \\Delta \\vec{w}\n",
    "+ O(\\|\\Delta \\vec{w}\\|^3)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9d6a86-5956-42fc-a09b-5711f578142f",
   "metadata": {},
   "source": [
    "#### Component expansion of gradient and hessian\n",
    "$$\n",
    "\\Delta E\n",
    "= \\textcolor{blue}{\\sum_{i}\\frac{\\partial E}{\\partial w_i}\\,\\Delta w_i}\n",
    "+ \\textcolor{green}{\\tfrac{1}{2}\\sum_{i}\\sum_{j}\\,\\Delta w_i\n",
    "\\frac{\\partial^2 E}{\\partial w_i\\,\\partial w_j}\\,\n",
    "\\Delta w_j}\n",
    "+ \\textcolor{red}{O(\\|\\Delta \\vec{w}\\|^3)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\textcolor{blue}{\\sum_{i}\\frac{\\partial E}{\\partial w_i}\\,\\Delta w_i}\n",
    "+ \\textcolor{green}{\\tfrac{1}{2}\\sum_{i}\\frac{\\partial^2 E}{\\partial w_i^2}\\,(\\Delta w_i)^2}\n",
    "+ \\textcolor{green}{\\tfrac{1}{2}\\sum_{i}\\sum_{\\substack{j \\\\ i \\ne j}}\n",
    "\\frac{\\partial^2 E}{\\partial w_i\\,\\partial w_j}\\,\\Delta w_i\\,\\Delta w_j}\n",
    "+ \\textcolor{red}{O(\\|\\Delta \\vec{w}\\|^3)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a025d2-0c03-4ec6-b685-8fed91f9c4b1",
   "metadata": {},
   "source": [
    "#### Quadratic (Second-Order) Approximation\n",
    "$$\n",
    "\\approx \\textcolor{blue}{\\sum_{i}\\frac{\\partial E}{\\partial w_i}\\,\\Delta w_i}\n",
    "+ \\textcolor{green}{\\tfrac{1}{2}\\sum_{i}\\frac{\\partial^2 E}{\\partial w_i^2}\\,(\\Delta w_i)^2}\n",
    "+ \\textcolor{green}{\\tfrac{1}{2}\\sum_{i}\\sum_{\\substack{j \\\\ i \\ne j}}\n",
    "\\frac{\\partial^2 E}{\\partial w_i\\,\\partial w_j}\\,\\Delta w_i\\,\\Delta w_j}\n",
    "+ \\textcolor{red}{\\cancel{O(\\|\\Delta \\vec{w}\\|^3)}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc0b781-ded3-420d-96aa-92f031ff1ee4",
   "metadata": {},
   "source": [
    "#### Extremal Approximation\n",
    "$$\n",
    "\\nabla E \\approx 0 \\quad \\Rightarrow \\quad \n",
    "\\textcolor{blue}{\\sum_i \\frac{\\partial E}{\\partial w_i}\\,\\Delta w_i \\approx 0}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\approx \\textcolor{blue}{\\cancel{\\sum_{i}\\frac{\\partial E}{\\partial w_i}\\,\\Delta w_i}}\n",
    "+ \\textcolor{green}{\\tfrac{1}{2}\\sum_{i}\\frac{\\partial^2 E}{\\partial w_i^2}\\,(\\Delta w_i)^2}\n",
    "+ \\textcolor{green}{\\tfrac{1}{2}\\sum_{i}\\sum_{\\substack{j \\\\ i \\ne j}}\n",
    "\\frac{\\partial^2 E}{\\partial w_i\\,\\partial w_j}\\,\\Delta w_i\\,\\Delta w_j}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e2ce08-6f70-4c12-8ee4-bebeee02baa9",
   "metadata": {},
   "source": [
    "#### Diagonal Approximation\n",
    "$$\n",
    "\\frac{\\partial^2 E}{\\partial w_i\\,\\partial w_j} \\approx 0 \\quad \\text{for } i \\ne j,\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\approx \\textcolor{green}{\\tfrac{1}{2}\\sum_{i}\\frac{\\partial^2 E}{\\partial w_i^2}\\,(\\Delta w_i)^2}\n",
    "+ \\textcolor{green}{\\tfrac{1}{2}\\cancel{\\sum_{i}\\sum_{\\substack{j \\\\ i \\ne j}}\n",
    "\\frac{\\partial^2 E}{\\partial w_i\\,\\partial w_j}\\,\\Delta w_i\\,\\Delta w_j}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef7d6d5-cdb6-47be-b6cf-20abda0d906e",
   "metadata": {},
   "source": [
    "#### Changing weights ($\\vec{w}$) by  $\\Delta \\vec{w}$ approximately changes $E$ by\n",
    "$$\n",
    "\\boxed{\n",
    "\\Delta E \\approx \n",
    "\\tfrac{1}{2}\\sum_{i}\\frac{\\partial^2 E}{\\partial w_i^2}(\\Delta w_i)^2\n",
    "}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371664bc-ef39-44af-9475-7d5d3841063d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deea5fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2cb863",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize((28, 28)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_set = datasets.USPS(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_set, batch_size=256, shuffle=True)\n",
    "\n",
    "test_set = datasets.USPS(root='./data', train=False, download=True, transform=transform)\n",
    "test_loader = DataLoader(test_set, batch_size=256, shuffle=False)\n",
    "\n",
    "print(train_set)\n",
    "print(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82804e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class USPSClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.H1 = nn.Conv2d(1, 4, 5)\n",
    "        self.H2 = nn.AvgPool2d(2, 2)\n",
    "        self.H3 = nn.Conv2d(4, 12, 5)\n",
    "        self.H4 = nn.AvgPool2d(2, 2)\n",
    "        self.output = nn.Linear(12 * 4 * 4, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.H1(x)\n",
    "        x = self.H2(x)          \n",
    "        x = self.H3(x)\n",
    "        x = self.H4(x)  \n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.tanh(self.output(x))\n",
    "        return x\n",
    "\n",
    "model = USPSClassifier()\n",
    "print(\"Parameter count:\", sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff422686",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def train_mse(model, optimizer, train_loader, epochs=30, mask_dict=None):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "\n",
    "            # convert integer labels → ±1 one-hot targets\n",
    "            targets = torch.full((target.size(0), 10), -1.0, device=DEVICE)\n",
    "            targets.scatter_(1, target.view(-1, 1), 1.0)\n",
    "\n",
    "            loss = F.mse_loss(output, targets)\n",
    "            loss.backward()\n",
    "\n",
    "            # --- Mask gradients of pruned weights ---\n",
    "            # Note: requires_grad is for whole tensor not individual weights\n",
    "            if mask_dict is not None:\n",
    "                for name, param in model.named_parameters():\n",
    "                    if \"weight\" in name and name in mask_dict:\n",
    "                        if param.grad is not None:\n",
    "                            param.grad.mul_(mask_dict[name])\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            # --- Apply mask to weights ---\n",
    "            if mask_dict is not None:\n",
    "                with torch.no_grad():\n",
    "                    for name, param in model.named_parameters():\n",
    "                        if \"weight\" in name and name in mask_dict:\n",
    "                            param.mul_(mask_dict[name])\n",
    "\n",
    "            total_loss += loss.item() * data.size(0)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {total_loss/len(train_loader.dataset):.4f}\")\n",
    "\n",
    "def test_mse(model, test_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "            output = model(data)\n",
    "\n",
    "            targets = torch.full((target.size(0), 10), -1.0, device=DEVICE)\n",
    "            targets.scatter_(1, target.view(-1, 1), 1.0)\n",
    "\n",
    "            loss = F.mse_loss(output, targets, reduction=\"sum\")\n",
    "            total_loss += loss.item()\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df10bdf-5762-4cfb-b878-9e41b246657b",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6741205",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "model = USPSClassifier().to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Train baseline\n",
    "train_mse(model, optimizer, train_loader, epochs=50)\n",
    "baseline_mse = test_mse(model, test_loader)\n",
    "print(f\"Test unreduced baseline MSE: {baseline_mse}\")\n",
    "\n",
    "# Clone baseline\n",
    "baseline_state = copy.deepcopy(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb74edae-3933-4dad-a043-8374b14bcabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_remaining_params(model):\n",
    "    remaining = 0\n",
    "    total = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"weight\" in name:\n",
    "            total += param.numel()\n",
    "            remaining += (param.detach() != 0).sum().item()\n",
    "    return remaining, total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75fac81-fb39-4921-853e-f1a0c77ed31f",
   "metadata": {},
   "source": [
    "### Optimal Brain Damage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bf1dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_pruned_mask_obd(model, hessian_diag, prune_fraction=0.2, mask_dict=None):\n",
    "    if mask_dict is None:\n",
    "        mask_dict = {name: torch.ones_like(param) for name, param in model.named_parameters() if \"weight\" in name}\n",
    "\n",
    "    # --- Compute saliencies for unpruned weights ---\n",
    "    saliency_dict = {}\n",
    "    saliencies = []\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if \"weight\" in name:\n",
    "            h_diag = hessian_diag[name]\n",
    "            sal = 0.5 * (param.detach() ** 2) * h_diag\n",
    "            sal = sal * mask_dict[name]\n",
    "    \n",
    "            saliency_dict[name] = sal          # <-- SAME FORMAT AS MASK_DICT\n",
    "            saliencies.append(sal.view(-1))\n",
    "    \n",
    "    saliencies = torch.cat(saliencies)\n",
    "    \n",
    "    # --- Determine saliency threshold ---\n",
    "    # We only consider the unpruned weights (saliencies != 0) for pruning\n",
    "    unpruned_saliencies = saliencies[saliencies != 0]\n",
    "    if unpruned_saliencies.numel() == 0:\n",
    "        return mask_dict\n",
    "\n",
    "    k = int(prune_fraction * unpruned_saliencies.numel())\n",
    "    if k <= 0:\n",
    "        return mask_dict\n",
    "    threshold = torch.topk(unpruned_saliencies, k, largest=False).values.max()\n",
    "\n",
    "    # --- Update mask cumulatively ---\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"weight\" in name:\n",
    "            h_diag = hessian_diag[name]\n",
    "            sal = 0.5 * (param.detach() ** 2) * h_diag\n",
    "            # We prune if the saliency is <= threshold and the weight is currently unpruned (mask=1)\n",
    "            # But note: we don't want to prune again the already pruned ones, but the mask already zeros them out.\n",
    "            new_mask = ((sal > threshold).float()) * mask_dict[name]\n",
    "            mask_dict[name] = new_mask\n",
    "    return mask_dict, saliency_dict\n",
    "\n",
    "def compute_hessian_diag(model, data_loader):\n",
    "    hessian_diag = {n: torch.zeros_like(p) for n, p in model.named_parameters() if \"weight\" in n}\n",
    "    model.eval()\n",
    "    \n",
    "    total = 0\n",
    "    for inputs, targets in data_loader:\n",
    "        inputs, targets = inputs.to(next(model.parameters()).device), targets.to(next(model.parameters()).device)\n",
    "        model.zero_grad()\n",
    "\n",
    "    \n",
    "        outputs = model(inputs)\n",
    "        targets_vec = torch.full(\n",
    "            (targets.size(0), outputs.size(1)),\n",
    "            -1.0,\n",
    "            device=targets.device\n",
    "        )\n",
    "        targets_vec.scatter_(1, targets.view(-1, 1), 1.0)\n",
    "        loss = F.mse_loss(outputs, targets_vec)\n",
    "        # print(loss)\n",
    "\n",
    "        grads = torch.autograd.grad(loss, [p for n, p in model.named_parameters() if \"weight\" in n], create_graph=True)\n",
    "        for (name, param), g in zip([(n, p) for n,p in model.named_parameters() if \"weight\" in n], grads):\n",
    "            if g is not None:\n",
    "                grad2 = torch.autograd.grad(g, param, grad_outputs=torch.ones_like(g), retain_graph=True)[0]\n",
    "                if grad2 is not None:\n",
    "                    hessian_diag[name] += grad2.detach()\n",
    "        total += 1\n",
    "\n",
    "    for name in hessian_diag:\n",
    "        hessian_diag[name] /= total\n",
    "    return hessian_diag\n",
    "\n",
    "# --- Reload baseline for OBD pruning ---\n",
    "model.load_state_dict(copy.deepcopy(baseline_state))\n",
    "\n",
    "# --- Initialize pruned-mask dictionary to all ones ---\n",
    "mask_dict = {name: torch.ones_like(param) for name, param in model.named_parameters() if \"weight\" in name}\n",
    "\n",
    "# --- Initialize stats to save ---\n",
    "mse_list = [baseline_mse]\n",
    "remaining_params = sum(mask.sum().item() for mask in mask_dict.values())\n",
    "remaining_params_list = [remaining_params]\n",
    "\n",
    "# --- Iterative pruning loop ---\n",
    "PRUNING_STEPS = 12\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "for prune_iter in range(PRUNING_STEPS):\n",
    "    print(f\"\\n--- Pruning Iteration {prune_iter+1}/{PRUNING_STEPS} ---\")\n",
    "\n",
    "    # Compute diagonal of Hessian\n",
    "    h_diag = compute_hessian_diag(model, train_loader)\n",
    "\n",
    "    # Prune lowest-saliency weights via OBD criterion\n",
    "    mask_dict, saliencies = update_pruned_mask_obd(model, h_diag, prune_fraction=0.10, mask_dict=mask_dict)\n",
    "\n",
    "    # Fine-tune the pruned model\n",
    "    train_mse(model, optimizer, train_loader, epochs=10, mask_dict=mask_dict)\n",
    "\n",
    "    # Evaluate\n",
    "    mse = test_mse(model, test_loader)\n",
    "\n",
    "    # Compute and save sparsity stats\n",
    "    remaining_params, total_params = count_remaining_params(model)\n",
    "    mse_list.append(mse)\n",
    "    remaining_params_list.append(remaining_params)\n",
    "    \n",
    "    print(f\"remaining_params={remaining_params}/{total_params}, mse={mse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432b11ce-9eb4-4351-8837-9dee64fb8e9b",
   "metadata": {},
   "source": [
    "### Magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b37932a-5405-4aeb-8e0c-cce646f4d3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_prune_mask_mag(model, mask_mag, prune_fraction=0.10):\n",
    "    \"\"\"\n",
    "    Computes global magnitude threshold among *unpruned* weights,\n",
    "    updates the mask cumulatively, and returns the new mask.\n",
    "    \"\"\"\n",
    "    # --- Compute global magnitude threshold (among unpruned weights only) ---\n",
    "    all_weights = torch.cat([\n",
    "        (param.detach().abs() * mask_mag[name]).flatten()\n",
    "        for name, param in model.named_parameters()\n",
    "        if \"weight\" in name\n",
    "    ])\n",
    "\n",
    "    unpruned = all_weights[all_weights != 0]\n",
    "    if unpruned.numel() == 0:\n",
    "        return mask_mag  # nothing left to prune\n",
    "\n",
    "    k = int(prune_fraction * unpruned.numel())\n",
    "    if k <= 0:\n",
    "        return mask_mag\n",
    "\n",
    "    threshold = torch.topk(unpruned, k, largest=False).values.max()\n",
    "\n",
    "    # --- Update mask cumulatively ---\n",
    "    new_mask_mag = {}\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"weight\" in name:\n",
    "            new_mask = ((param.detach().abs() > threshold).float()) * mask_mag[name]\n",
    "            new_mask_mag[name] = new_mask\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    return new_mask_mag\n",
    "\n",
    "# --- Reload baseline for magnitude pruning ---\n",
    "model.load_state_dict(copy.deepcopy(baseline_state))\n",
    "\n",
    "# --- Initialize pruned-mask dictionary to all ones ---\n",
    "mask_mag = {name: torch.ones_like(param) for name, param in model.named_parameters() if \"weight\" in name}\n",
    "\n",
    "# --- Initialize stats to save ---\n",
    "mse_list_mag = [baseline_mse]\n",
    "remaining_params_mag = sum(mask.sum().item() for mask in mask_mag.values())\n",
    "remaining_params_list_mag = [remaining_params_mag]\n",
    "\n",
    "# --- Iterative pruning loop ---\n",
    "PRUNING_STEPS = 12\n",
    "optimizer_mag = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "for prune_iter in range(PRUNING_STEPS):\n",
    "    print(f\"\\n--- Magnitude Pruning Iteration {prune_iter+1}/{PRUNING_STEPS} ---\")\n",
    "\n",
    "    mask_mag = update_prune_mask_mag(model, mask_mag, prune_fraction=0.10)\n",
    "\n",
    "    # --- Fine-tune the pruned model ---\n",
    "    train_mse(model, optimizer_mag, train_loader, epochs=10, mask_dict=mask_mag)\n",
    "\n",
    "    # --- Evaluate ---\n",
    "    mse = test_mse(model, test_loader)\n",
    "\n",
    "    # --- Compute and save sparsity stats ---\n",
    "    remaining_params_mag, total_params = count_remaining_params(model)\n",
    "    mse_list_mag.append(mse)\n",
    "    remaining_params_list_mag.append(remaining_params_mag)\n",
    "    print(f\"remaining_params={remaining_params_mag}/{total_params}, mse={mse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1efbb36-cb05-4410-88cb-ac2aa988198a",
   "metadata": {},
   "source": [
    "### Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cd12b5-f3ef-4055-9100-a2d4e5ff4181",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_mse_db_comparison(mse_list_obd, remaining_obd, mse_list_mag, remaining_mag):\n",
    "    mse_list_obd = torch.tensor(mse_list_obd, dtype=torch.float32)\n",
    "    mse_list_mag = torch.tensor(mse_list_mag, dtype=torch.float32)\n",
    "\n",
    "    baseline_obd = mse_list_obd[0]\n",
    "    baseline_mag = mse_list_mag[0]\n",
    "\n",
    "    mse_db_obd = 10 * torch.log10(mse_list_obd / baseline_obd)\n",
    "    mse_db_mag = 10 * torch.log10(mse_list_mag / baseline_mag)\n",
    "\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(remaining_obd, mse_db_obd, marker='o', linestyle='-', linewidth=2, label=\"OBD\")\n",
    "    plt.plot(remaining_mag, mse_db_mag, marker='s', linestyle='--', linewidth=2, label=\"Magnitude\")\n",
    "\n",
    "    plt.xlabel(\"Parameters\")\n",
    "    plt.ylabel(\"logMSE\")\n",
    "    plt.title(\"OBD vs Magnitude Pruning\")\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Call with your data\n",
    "plot_mse_db_comparison(mse_list, remaining_params_list, mse_list_mag, remaining_params_list_mag)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
