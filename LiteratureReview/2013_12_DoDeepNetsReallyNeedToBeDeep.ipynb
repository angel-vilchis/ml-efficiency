{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e999338-cb52-4e74-a778-5e230a60e89e",
   "metadata": {},
   "source": [
    "# [Do Deep Nets Really Need to be Deep?](https://arxiv.org/pdf/1312.6184)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be75ca51-04dd-4259-9582-5edd22622bf1",
   "metadata": {},
   "source": [
    "## Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b5dc40-3866-4dc6-a69f-ea7c15504451",
   "metadata": {},
   "source": [
    "Since it has been shown we can mimic the function learned by complex model with a small net, the function learned by complex model wasn't truly too complex to be learned by the small net\n",
    "* Showing that shallow models are capable of learning the same function as deep nets debunks the myth that function learned by deep net has to be deep. <br><br>\n",
    "    \n",
    "Shallow and wide models are slower for learning since there are many highly correlated features, so gradient descent converges slowly\n",
    "* One remedy is linear bottleneck (weight matrix factorization), which only increases speed and doesn't increase representational power <br><br>\n",
    "\n",
    "Why training on teacher model's prediction could be better than training directly on original dataset can be due to:\n",
    "* Teacher model can eliminate label errors by predicting those correctly (from generalization)\n",
    "* Teacher model soft targets provide more information than hard targets such as confusable classes <br><br>\n",
    "    \n",
    "Model compression works best when unlabeled data set is much larger than train set (to reduce gap between teacher and student) and when unlabeled examples aren't training points (teacher model is more likely to have overfit these)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b10c11-46f4-4005-a554-37f027ff699f",
   "metadata": {},
   "source": [
    "## Soft Targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b535cf6-0e25-4553-a495-49d6374dcbd7",
   "metadata": {},
   "source": [
    "It is better to train a student model on logits since different logits can map to same distribution when using softmax (technically losing information the complex model learned)\n",
    "* Also, softmax can lead to few large values relative to others, which would cause cross entropy to focus on them, ignoring others <br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fdb6b36-4937-423a-9fc6-9034f5c9810a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.0611e-09, 4.5398e-05, 9.9995e-01])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.nn.functional.softmax(torch.tensor([-10.0, 0.0, 10.0]), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9f4f3e5-34f1-42cb-a8c5-b54aa1b91bd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.0611e-09, 4.5398e-05, 9.9995e-01])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.softmax(torch.tensor([10.0, 20.0, 30.0]), dim=-1) # Different logits same softmax output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf74e32-ee11-40c7-bd56-98792523d052",
   "metadata": {},
   "source": [
    "Say target is $[3.0385e^{-7}, 6.6928e^{-3}, 9.9331e^{-1}]$ and prediction is $[\\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3}]$\n",
    "\n",
    "$CE_{Loss} \\approx -[3.0e^{-7} \\ log(\\frac{1}{3}) + 6.7e^{-3} \\ log(\\frac{1}{3}) + 9.9e^{-1} \\ log(\\frac{1}{3})] = $\n",
    "\n",
    "We can see most of the loss would come from largest target so model would focus on getting that right and ignoring others targets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3165940-39d9-486a-9533-431b2ea0dbbb",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f49f4bb8-fcb5-494f-9549-92077abfbba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61a65338-4656-4a82-aa19-29f11c655baa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/AirlineSatisfaction/prepared.csv')\n",
    "target_column = \"isSatisfied\"\n",
    "X = df.drop(columns=[target_column])\n",
    "y = df[target_column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a93cdf8c-0fe4-4faa-b91d-a9e1d81025a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, X_unlabeled, y, y_unlabeled = train_test_split(X, y, test_size=0.1, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "654f9029-f7ae-4f7d-85a3-17d8866607c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112aabe6-f886-4008-af86-e9178ef35213",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b66b8605-0750-45e1-8121-0bb2562e82e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class LargeNet(nn.Module):\n",
    "    def __init__(self, input_size=(24), output_size=1, num_neurons=32, p=0.5):\n",
    "        super(LargeNet, self).__init__()\n",
    "        self.p = p\n",
    "        input_size = np.prod(input_size)\n",
    "        self.fc1 = nn.Linear(input_size, num_neurons)\n",
    "        self.fc2 = nn.Linear(num_neurons, num_neurons)\n",
    "        self.out = nn.Linear(num_neurons, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        fc1 = F.relu(self.fc1(x))\n",
    "        fc2 = F.dropout(fc1, p=self.p)\n",
    "        logits = self.out(fc2)\n",
    "        return logits\n",
    "\n",
    "class SmallNet(nn.Module):\n",
    "    def __init__(self, input_size=(24), output_size=1, num_neurons=5):\n",
    "        super(SmallNet, self).__init__()\n",
    "        input_size = np.prod(input_size)\n",
    "        self.fc1 = nn.Linear(input_size, num_neurons)\n",
    "        self.fc2 = nn.Linear(num_neurons, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        fc1 = F.relu(self.fc1(x))\n",
    "        logits = self.fc2(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0328a490-a414-4195-878a-237d3e005f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def train_model(model, optimizer, num_epochs=20):\n",
    "    model = model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set the model to training mode\n",
    "        running_loss = 0.0\n",
    "    \n",
    "        progress_bar = tqdm(total=len(train_loader))\n",
    "    \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.view(inputs.shape[0], -1).to(device), labels.to(device)\n",
    "            optimizer.zero_grad()  # Zero the gradients\n",
    "            \n",
    "            outputs = model(inputs)  # Forward pass\n",
    "            loss = criterion(outputs.view(-1), labels)  # Compute the loss\n",
    "            loss.backward()  # Backward pass\n",
    "            optimizer.step()  # Update the weights\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "            progress_bar.update(1)\n",
    "            progress_bar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "        all_preds, all_labels = test_model(model)\n",
    "        num_instances = len(all_preds)\n",
    "        correct = (all_preds == all_labels).sum()\n",
    "        accuracy =  correct / num_instances\n",
    "        errors = num_instances - correct\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {accuracy}, Errors: {errors}\")\n",
    "\n",
    "def test_model(model):\n",
    "    model.eval()\n",
    "    progress_bar = tqdm(total=len(test_loader))\n",
    "    \n",
    "    all_preds, all_labels = [], []\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.view(inputs.shape[0], -1).to(device), labels.to(device)\n",
    "        \n",
    "        outputs = model(inputs)  # Forward pass\n",
    "        outputs = nn.functional.sigmoid(outputs)\n",
    "        preds = outputs > 0.5\n",
    "        all_preds += list(preds)\n",
    "        all_labels += list(labels)\n",
    "        progress_bar.update(1)\n",
    "    all_preds, all_labels = torch.tensor(all_preds), torch.tensor(all_labels)\n",
    "    return all_preds, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b95843a4-f380-4875-8b71-d9bdc462e6d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LR = 1e-3\n",
    "large_model = LargeNet()\n",
    "optimizer = optim.Adam(large_model.parameters(), lr=LR)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "device =  \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7c551baf-268e-43f6-bc5a-df5b0ec4020f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to tensor\n",
    "X_train_tensor = torch.tensor(X_train.to_numpy(), dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.to_numpy(), dtype=torch.float32)\n",
    "\n",
    "# Create DataLoader for batching\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7f7abe2b-7bac-4e40-9d96-0dacff38af09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to tensor\n",
    "X_test_tensor = torch.tensor(X_test.to_numpy(), dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.to_numpy(), dtype=torch.float32)\n",
    "\n",
    "# Create DataLoader for batching\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e0342f99-3ed2-403b-b0fa-dc1a6f340182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7446e9e2b65242269ec04a953c647457",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1169 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c6d7ab932d44b9fbed23b38c6ce1f64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/293 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 62.0892, Accuracy: 0.5431213974952698, Errors: 8545\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49a6eaab373e4e03bb02c37cbffc875e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1169 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d96e1a77cf49412b8b49fdd0699557c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/293 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20, Loss: 0.8896, Accuracy: 0.5009891390800476, Errors: 9333\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d55e5ce9c2848268c7d06abd0b9fc01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1169 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03e95811ba464f64bc1330166d5ec8bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/293 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20, Loss: 0.8104, Accuracy: 0.472223699092865, Errors: 9871\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b3da28bdc634bc3b781a710cbe9462a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1169 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[83], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_model(large_model, optimizer)\n",
      "Cell \u001b[1;32mIn[79], line 13\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, optimizer, num_epochs)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m     12\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mview(inputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 13\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# Zero the gradients\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(inputs)  \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), labels)  \u001b[38;5;66;03m# Compute the loss\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\efficiency\\Lib\\site-packages\\torch\\_compile.py:24\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\efficiency\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:451\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    449\u001b[0m prior \u001b[38;5;241m=\u001b[39m set_eval_frame(callback)\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    453\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\efficiency\\Lib\\site-packages\\torch\\optim\\optimizer.py:825\u001b[0m, in \u001b[0;36mOptimizer.zero_grad\u001b[1;34m(self, set_to_none)\u001b[0m\n\u001b[0;32m    823\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    824\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m set_to_none:\n\u001b[1;32m--> 825\u001b[0m         p\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    826\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mgrad_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model(large_model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c2521f-8b15-47c3-8a3e-09eb411cebb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
